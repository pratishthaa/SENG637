**SENG 637 - Dependability and Reliability of Software Systems**

**Lab. Report #3 – Code Coverage, Adequacy Criteria and Test Case Correlation**

| Group: 4      |
|-----------------|
| Student 1 Arpita Chowdhury                |   
| Student 2 Fadila Abdulai Hamid             |   
| Student 3 Kumkum Akter             |   
| Student 4 Niloofar Sharifisadr              |
| Student 5 Pratishtha Pratishtha |  


**Table of Contents**

[1 Introduction	1](#intro)

[2 Manual data-flow coverage calculations for X and Y methods](#man)

[3 A detailed description of the testing strategy for the new unit test](#testst)

[4 A high level description of five selected test cases you have designed using coverage information, and how they have increased code coverage](#hld)

[5 A detailed report of the coverage achieved of each class and method (a screen shot from the code cover results in green and red color would suffice)](#det)

[6 Pros and Cons of coverage tools used and Metrics you report](#pcon)

[7 A comparison on the advantages and disadvantages of requirements-based test generation and coverage-based test generation.](#diffe)

[8 A discussion on how the team work/effort was divided and managed](#team)

[9 Any difficulties encountered, challenges overcome, and lessons learned from performing the lab](#diff)

[10 Comments/feedback on the lab itself](#comm)


# 1 Introduction <a name="intro"></a>

Software testing is a critical component of the software development process, aiming to ensure the reliability, correctness, and robustness of software systems. In this assignment, our focus is on unit testing, specifically using JUnit, a popular testing framework in the Java ecosystem. Through this assignment, we delved into the principles of testing, with a particular emphasis on **white-box coverage criteria**, which help determine the adequacy of a test suite based on its coverage of the underlying code.

Our goal in this assignment is to write new test cases and utilize the test cases from the previous assignment to meet a predetermined code coverage. The `JFreeChart` package, which allows us to create charts in various ways, is the **system under test (SUT)**. It is an open-source Java framework available for free that calculates, creates, and displays charts. Numerous chart types are supported by this framework, such as histograms, pie charts, bar charts, line charts, and several others. The percentage of the source code that is covered while the test suite executes is the code coverage. JFreeChart makes it easy for developers to display professional charts in their applications. In Project 2, we created black-box test cases for ten distinct JFreeChart methods; five of them from the `Range class` and the other five from the `DataUtilities class`. 

Code coverage tools play a vital role in this process by providing insights into the extent to which the code is exercised by the test suite. Various coverage metrics, such as statement, branch, and condition coverages, aid in assessing the comprehensiveness of testing efforts. Additionally, we explored data-flow coverage criteria, such as DU pairs coverage, to deepen their understanding of how coverage tools work. In this assignment, we employ these test cases and test for their code coverage. We hope to achieve a minimum of 
  - 90% statement coverage.
  - 70% branch coverage.
  - 60%  method coverage.

For methods that we don't attain this defined code coverage, we have written new test cases to increase test coverage. For methods we obtain the minimum defined coverage, we have written more test cases for practice and improve ourselves at Software Testing.

In this report, throughout all sections, we will provide a detailed account of our testing strategy, including the selection of coverage metrics, the tools used for measuring coverage, and the challenges encountered during the testing process. We will discuss the advantages and limitations of different coverage tools, reflecting on their integration with IDEs, user-friendliness, and effectiveness in identifying gaps in test coverage apart from manual data flow coverage.

# 2 Manual data-flow coverage calculations for X and Y methods <a name="man"></a>

**Data flow**
![DataUtilities drawio (2)](https://github.com/pratishthaa/SENG637/assets/17180836/5b66cd9c-c51f-4dda-8169-1cee84c88d4f)


**Def-Use Table**

Given the source code for `calculateColumnTotal` and the outlined test cases, we identify the key variables (data, column, total, rowCount, r, n) and their points of definition and use. We construct the table as follows:

| Variable | Defined at Node | dcu (Definition-Clear Use) | dpu (Definition-Predicate Use) |
|----------|-----------------|----------------------------|--------------------------------|
| data     | 1               | {5, 9}                     |                                |
| column   | 1               | {5,9}                        |                                |
| total    | 2               | {7, 11, 12}                     |                                |
| total    | 7               | {7, 11, 12}                     |                                |
| rowCount | 3               |                            | {(4, 5), (4, 8), (8,12) , (8,9)}               |
| n        | 9               | {7,11}                        | {(10,8), (10, 11)}               |
| n        | 5               | {7,11}                     |  {(6,4) , (6,7)}                              |


# 3 A detailed description of the testing strategy for the new unit test <a name="testst"></a>

Text…

# 4 A high level description of five selected test cases you have designed using coverage information, and how they have increased code coverage <a name="hld"></a>

**Range.Contains method**

We enhanced `range.contains()` method, with the addition of three test cases. This strategic inclusion aimed at addressing boundary conditions and the handling of `NaN` values, which propelled our code coverage from 75% to 87.5%. Here's a concise overview of the improvements:

- **`testContains_BoundaryValueLower_ReturnsTrue`** ensures the lower boundary of a range (1.0 to 10.0) is correctly identified as being within the range. This test validates the accurate behavior of `contains()` when assessing the lowest limit, affirming that the method rightly includes the lower boundary value.

- **`testContains_BoundaryValueUpper_ReturnsTrue`** checks that the upper boundary of a range (1.0 to 10.0) is considered part of the range. Through this, we verify the `contains()` function's capability to recognize the upper limit as within bounds, reinforcing its reliability in edge-case evaluations.

- **`testContains_NaNValue`** examines the `contains()` method's response to `NaN` values by testing with a range from 1.0 to 10.0. This case confirms that `NaN` values are appropriately handled as external to any numeric range, showcasing the method's robustness against non-numeric inputs.

These additions significantly contribute to the thoroughness of our testing strategy, ensuring the `Range` class's `contains()` method operates reliably across standard, boundary, and exceptional value scenarios.

**DataUtilities.CalculateColoumnTotal method**

The test case additions targeted specific scenarios involving null values, empty data sets, and invalid column indices. As a result, our test coverage experienced a significant boost, reaching 62.5%. Below is a detailed breakdown of the newly integrated tests:

- **`calculateColumnTotalWithNullValues`** effectively demonstrates the method's capability to handle null values within the dataset. By mocking a `Values2D` object to return a mix of numeric values and a null, this test asserts that `calculateColumnTotal` correctly sums up the available numeric values, ignoring nulls, thereby ensuring robustness in real-world data scenarios.

- **`calculateColumnTotalForEmptyDataSet`** addresses the method's behavior when faced with an empty dataset. The setup involves a `Values2D` object configured to represent a dataset with zero rows. The outcome of this test confirms that the method returns a total of 0.0, which aligns with expected functionality, thus validating the method's handling of empty datasets.

- **`calculateColumnTotalForInvalidColumnIndex`** explores the method's response to an invalid column index. By setting up a scenario where the provided column index is beyond the dataset's bounds and expecting an `IndexOutOfBoundsException`, this test ensures that the method adequately safeguards against improper index values, enhancing error handling and security within the `DataUtilities` class

# 5 A detailed report of the coverage achieved of each class and method (a screen shot from the code cover results in green and red color would suffice) <a name="det"></a>

## Range.Contains before
![image](https://github.com/seng637-Winter/seng637-a3-niloofarsharifi/assets/17180836/471b0571-edcf-47c4-9c70-df148b343c6d)

## Range.Contains after
![image](https://github.com/seng637-Winter/seng637-a3-niloofarsharifi/assets/17180836/c683de39-f8b8-42fd-a0a8-0d97f811eb04)


## DataUtilities.CalculateColoumnTotal before
![image](https://github.com/seng637-Winter/seng637-a3-niloofarsharifi/assets/17180836/18f96795-52e5-4e74-ae40-b7171c866810)

## DataUtilities.CalculateColoumnTotal after
![image](https://github.com/seng637-Winter/seng637-a3-niloofarsharifi/assets/17180836/d0b37898-7ede-4a77-a8d3-b9fc38b7b4bf)


# 6 Pros and Cons of coverage tools used and Metrics you report <a name="pcon"></a>

For this assignment, we have used `Eclemma` as our coverage tool, as it is already integrated in Eclipse and thus is easily accessible.

## Advantages of EclEmma:

**Seamless Integration with Eclipse:** EclEmma's integration with the Eclipse IDE is seamless, offering developers(like us here) a familiar environment for code coverage analysis without needing external tools.

**Intuitive User Interface:** With its user-friendly interface, EclEmma provides developers with clear and easy-to-understand coverage results, enhancing productivity and reducing the learning curve.

**Visual Code Highlighting:** EclEmma highlights code directly within the Eclipse editor, making it visually evident which lines are covered by tests and which are not, facilitating quick identification of untested code segments.

**Diverse Coverage Metrics:** Supporting a range of coverage metrics, including branch, line and method coverage, EclEmma empowers developers to select the most relevant metrics for assessing the comprehensiveness of their test suites.

**Comprehensive Reporting:** EclEmma generates comprehensive coverage reports, offering detailed insights into the extent of test coverage across the codebase. These reports aid in identifying areas requiring additional testing efforts.

## Limitations of EclEmma:

**Compatibility Challenges:** EclEmma may encounter compatibility issues with specific versions of Eclipse or other plugins, potentially leading to conflicts or unstable development environment.

**Performance Overhead:** Instrumenting code for coverage analysis may introduce performance overhead, particularly in large or complex codebases, impacting the efficiency of the testing process.

**Limited Metric Support:** EclEmma may lack support for specific advanced coverage metrics, such as data flow coverage, limiting its suitability for comprehensive test coverage analysis in specific scenarios.

**IDE Dependency:** As EclEmma is tightly coupled with Eclipse, developers using other IDEs may not fully leverage its features, restricting its adoption among a broader software developers community.

**Missing Advanced Features:** Compared to standalone coverage tools, EclEmma may need more advanced features and customization options, constraining its flexibility for specialized testing requirements, beyond the ones covered.

In terms of the types of coverage that `Eclemma` provides, we mostly focused on Method, line and branch coverage. 

## **Pros and Cons of EclEmma's Method, Line, and Branch Coverage:**

### **Method Coverage:**

**Pros:**

**High-Level View:** Provides a broad overview of tested methods, aiding in identifying untested areas efficiently.

**Simplicity:** Easy to understand and implement, suitable for quick assessments of test suite effectiveness.

**Focus on Core Functionality:** Prioritizes testing of essential functionalities within the codebase.

**Cons:**

**Limited Granularity:** Doesn't differentiate between different execution paths within methods.

**Inadequate for Complex Methods:** May not capture all execution scenarios in complex methods.

### **Line Coverage:**

**Pros:**

**Granular Insights:** Offers detailed insights into tested code lines, facilitating pinpointing areas requiring additional testing.

**Accurate Assessment:** Provides a more accurate assessment of test suite effectiveness compared to method coverage.

**Detection of Dead Code:** Helps detect and eliminate redundant or unreachable code segments.

**Cons:**

**Overemphasis on Syntax:** May prioritize syntax coverage over functional behavior.

**Complexity Overhead:** Achieving high line coverage may increase testing effort and maintenance burden.

### **Branch Coverage:**

**Pros:**

**Path Exploration:** Explores different execution paths within conditional statements, ensuring comprehensive testing.

**Detection of Missing Conditions:** Identifies gaps in test coverage within conditional statements.

**Enhanced Quality Assurance:** Reduces the likelihood of overlooking critical decision points or logic errors.

**Cons:**

**Complexity Management:** Testing intricate branching structures may require extensive effort.

**False Sense of Completeness:** Does not guarantee exhaustive testing within branches, risking incomplete coverage.








# 7 A comparison on the advantages and disadvantages of requirements-based test generation and coverage-based test generation. <a name="diffe"></a>

Text…

# 8 A discussion on how the team work/effort was divided and managed <a name="team"></a>

Each of the five members extended their work from assignment 2 where each one of us had written test cases for one method of the `Range class` and one method of the `DataUtilities class` that involved using jMock. We first used `Eclemma` to do test coverage(line, method, and branch coverage) for the test cases from assignment 2. After that, we developed more test cases to improve the different test coverages to meet the minimum value requirements and if they were already met by assignment 2 test cases, we wrote more test cases to improve ourselves at Software Testing. Group peer review was done after the individual testing to ensure that the quality of work abides by the scope of the test plan and for a fresh perspective on the designed test cases. The below table summarizes the distribution of development of test cases.

| API Range Method                                      | Tester                | 
| ----------------------------------------------- | --------------------- |
| Range.intersects(double, double)               | Arpita Chowdhury      |
| Range.getCentralValue()      | Fadila Abdulai Hamid |
| Range.expandtoinclude(Range range, double value)| Kumkum Akter          |
| Range.contains(double value)                         | Niloofar Sharifisadr  |
| Range.combine(Range range1, Range range2)      | Pratishtha Pratishtha |

| API DataUtilities Method                                      | Tester                | 
| ----------------------------------------------- | --------------------- |
| DataUtilities.createNumberArray(double[] data)               | Arpita Chowdhury      |
| DataUtilities.getCumulativePercentages(keyedValues data)      | Fadila Abdulai Hamid |
| DataUtilities.createNumberArray2D(double[] data)| Kumkum Akter          |
| DataUtilities.calculateColumnTotal(Values2D data, int coulmn)                         | Niloofar Sharifisadr  |
| DataUtilities.calculateRowTotal(Values2D data, int row)     | Pratishtha Pratishtha |

# 9 Any difficulties encountered, challenges overcome, and lessons learned from performing the lab <a name="diff"></a>

One of the challenges I faced was my test file name not matching the expected file name, however, that error was resolved with the help of a group mate. Also, trying to debug online via a WhatsApp group chat was sometimes frustrating, as it was not available whenever you needed help, and the assignment was not completed in one meeting, which made the group resolve to a WhatsApp chart.
Despite all the hindrances, we also understood various methodologies, such as white-box coverage requirements. Expertise in utilizing various testing tools to assess test adequacy, create test cases to enhance code coverage, comprehend the advantages and disadvantages of using code coverage tools to measure test adequacy, and learn how to compute data-flow coverage manually. 
Moreover, we utilized EclEmma and JaCoCo tools for  code coverage. First, we become acquainted with the SUT. The three coverage metrics—the statement, branch, and method were covered. We split up the job amongst us, which ultimately proved beneficial as we covered every measure. 
We split the work amongst us, which ultimately proved beneficial as we covered every measure. Despite working partly virtual, we collaborated to conduct testing in which we individually applied critical thinking to identify edge circumstances by writing more new test cases. Teamwork, code commits via GIT, and industrial defect tracking systems, procedures, and practices are just a few of the software engineering skills we gained.

# 10 Comments/feedback on the lab itself <a name="comm"></a>

The requirements for this project were elucidated in detail in the assignment assignment guideline. We grasped the content of the assignment by reading the summary and introduction, which also helped us to emphasize the objectives since they showed how the work would be completed. The detailed setup instructions with illustrations for the test cases and artifacts from the assignment guidelines enhance our understanding. The assignment's expected outcome was also stated in detail. 

